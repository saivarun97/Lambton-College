{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa189ca",
   "metadata": {},
   "source": [
    "## AML-3204 Social Media Analytics \n",
    "\n",
    "## Project Title: Collaborative Filtering-based vs Hybrid Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94da56",
   "metadata": {},
   "source": [
    "### Group Members\n",
    "\n",
    "- SAI VARUN KOLLIPARA - C0828403\n",
    "- PRAMOD REDDY GURRALA – C0850493\n",
    "- DEEKSHA NAIKAP – C0835440\n",
    "- BHANU PRAKASH MAHADEVUNI – C0850515"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e013db3",
   "metadata": {},
   "source": [
    "### Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5e385e-2859-4581-9fa5-9d83ffe95338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import itertools\n",
    "import codecs\n",
    "import re\n",
    "# path is the folder were the csv files will be stored\n",
    "path = 'tweets'\n",
    "\n",
    "class Scrapper:\n",
    "    def __init__(self, keyword, dates = [], fixed_query = '', tweets_to_obtain_per_day = 100):\n",
    "        self.keyword = keyword\n",
    "        self.dates = dates\n",
    "        self.fixed_query = fixed_query\n",
    "        self.batches = int(tweets_to_obtain_per_day / 100)\n",
    "        # Creates the dataframe for the keyword\n",
    "        self.dataframe = pd.DataFrame()\n",
    "    \n",
    "    def file_name(self):\n",
    "        clean_keyword = self.keyword.replace('@', '').replace('#', '').replace('/','')\n",
    "        return f'{path}/tweets_{clean_keyword}.csv'\n",
    "\n",
    "    def check_file_existence(self):\n",
    "        return os.path.exists(self.file_name())\n",
    "        \n",
    "    def scrap_tweets(self, from_date):\n",
    "        to_date = from_date + datetime.timedelta(days = 1)\n",
    "        query = f'{self.fixed_query} {self.keyword} since:{from_date.strftime(\"%Y-%m-%d\")} until:{to_date.strftime(\"%Y-%m-%d\")}'\n",
    "        result = sntwitter.TwitterSearchScraper(query) \n",
    "        df = pd.DataFrame(itertools.islice(result.get_items(), 50))\n",
    "        if len(df) > 0:\n",
    "            df['keyword'] = self.keyword\n",
    "            #self.dataframe = self.dataframe.append(df[['id', 'url', 'date', 'content', 'keyword']], ignore_index=True)\n",
    "            self.dataframe = pd.concat([self.dataframe, df[['id', 'url', 'date', 'content', 'keyword']]])\n",
    "        \n",
    "    def get_tweets(self):\n",
    "        for from_date in self.dates:\n",
    "            self.scrap_tweets(from_date)           \n",
    "       \n",
    "    def save(self):\n",
    "        print(f'Saving {self.file_name()}')\n",
    "        self.dataframe.to_csv(self.file_name())\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            print(f'Loading {self.file_name()}')\n",
    "            self.dataframe = pd.read_csv(self.file_name(),engine='python' )\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f'Empty {self.file_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc928d",
   "metadata": {},
   "source": [
    "### Cleaner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8feb20f-8e45-4821-ba51-358ddaab47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "porter = PorterStemmer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "class Cleaner:\n",
    "    def __init__(self, df, features):\n",
    "        self.result = df\n",
    "        self.features = features\n",
    "    \n",
    "    def get_result(self):\n",
    "        return self.result\n",
    "    \n",
    "    def drop_duplicates(self):\n",
    "        df = self.result\n",
    "        #df.set_index('id', inplace=True)\n",
    "        df[\"content\"].fillna(\"0\", inplace = True)\n",
    "        #df = df.dropna()\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        df['index'] = np.arange(len(df))\n",
    "        df.set_index('index', inplace=True)\n",
    "        self.result = df[self.features]\n",
    "\n",
    "    def clean_data(self, text):\n",
    "        \n",
    "        #Remove emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                    u\"\\U0001F600-\\U0001F64F\"\n",
    "                                   u\"\\U0001F900-\\U0001F9FF\"\n",
    "                                   u\"\\U0001F000-\\U0002FFFF\"\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r' ', text)\n",
    "        \n",
    "        #Remove all URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        #Remove numbers\n",
    "        text = re.sub(r'[0-9]+', ' ', text)\n",
    "        #Remove all words with a lenght less than 3\n",
    "        text = re.sub(r'\\b\\w{1,2}\\b', ' ', text)\n",
    "        #Remove everything that is not a word or a space\n",
    "        text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "        #Replace underscore with space\n",
    "        #text = re.sub(r'[_]',' ',text)\n",
    "        #Remove punctuation\n",
    "        text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "        #Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        #Remove stop-words\n",
    "        text = [word for word in tokens if word not in cachedStopWords]\n",
    "        return text\n",
    "   \n",
    "    def ex_hashtags(self, text):\n",
    "        return list(part[1:] for part in text.split() if part.startswith('#'))\n",
    "    \n",
    "    def extract_hashtags(self):\n",
    "        self.result['hashtags']=self.result['content'].apply(lambda x: self.ex_hashtags(x))\n",
    "    \n",
    "    def ex_usernames(self, text):\n",
    "        return list(part[1:] for part in text.split() if part.startswith('@'))\n",
    "    \n",
    "    def extract_usernames(self):\n",
    "        self.result['usernames']=self.result['content'].apply(lambda x: self.ex_usernames(x))\n",
    "    \n",
    "    def extract_content(self):\n",
    "        # create a column with the cleaned tokens\n",
    "        self.result['content_clean'] = self.result['content'].apply(lambda x: self.clean_data(x))\n",
    "        # Drop rows with content_clean lenght < 1 (i.e, where there is less than 1 token)\n",
    "        self.result = self.result[self.result['content_clean'].map(len) > 1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6547c8c",
   "metadata": {},
   "source": [
    "### Sentimental Analysis on the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970ab4b2-bdc6-4ca9-9c03-d63806cc221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following class summarizes the functions to calculate de sentiment with vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.result = df\n",
    "    def get_simple_sentiment_score(self,sentence):\n",
    "        return analyzer.polarity_scores(sentence)\n",
    "    def get_sentiments(self):\n",
    "        self.result['sentiment_score'] = self.result['content'].apply(lambda t: self.get_simple_sentiment_score(t)['compound'])\n",
    "        self.result['sentiment'] = self.result['sentiment_score'].apply(lambda s: 'positive' if s > 0.05 else ('negative' if s < -0.05 else 'neutral'))\n",
    "    def get_result(self):        \n",
    "        return self.result\n",
    "    def mean_sentiment_by_keyword(self):\n",
    "        self.result = self.result.groupby('keyword', as_index=False)['sentiment_score'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ac8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
